{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57aab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus, BM25BuiltInFunction\n",
    "from typing import Literal, Optional\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "find_dotenv()\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class MetaData(BaseModel):\n",
    "    language: Literal[\"ja\", \"en\"]\n",
    "    domain: Optional[str] = None\n",
    "    section: Optional[str] = None\n",
    "    topic: Optional[str] = None\n",
    "    doc_type: Optional[Literal[\"policy\", \"manual\", \"faq\"]] = None\n",
    "\n",
    "\n",
    "# model = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-lite\")\n",
    "# emb_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", output_dimensionality=1536)\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "emb_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1536)\n",
    "\n",
    "MILVUS_URI = \"./rag_task.db\"\n",
    "\n",
    "\n",
    "def get_vectorstore(collection_name: str) -> Milvus:\n",
    "    vectorstore = Milvus(\n",
    "        embedding_function=emb_model,\n",
    "        collection_name=collection_name,\n",
    "        connection_args={\"uri\": MILVUS_URI},\n",
    "        index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    "    )\n",
    "    # builtin_function=BM25BuiltInFunction(output_field_names=\"sparse\"),\n",
    "    # text_field=\"text\",\n",
    "    # vector_field=[\"dense\", \"sparse\"],\n",
    "    print(f\"vectorstore successfully initialized for {collection_name}\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db72701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def mask_pii(text: str) -> str:\n",
    "    \"\"\"Mask Personally Identifiable Information\"\"\"\n",
    "    # Email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "    \n",
    "    # Phone numbers\n",
    "    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n",
    "    \n",
    "    # Credit card numbers\n",
    "    text = re.sub(r'\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b', '[CREDIT_CARD]', text)\n",
    "    \n",
    "    # Social Security Numbers\n",
    "    text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', '[SSN]', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6037cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "\n",
    "find_dotenv()\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "\n",
    "\n",
    "def ingest(file_paths: List[str], collection_name: str, metadata: MetaData):\n",
    "    documents: list[Document] = []\n",
    "    for file_path in file_paths:\n",
    "        docs = PDFMinerLoader(file_path).load()\n",
    "        documents.extend(docs)\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = file_path.split(\"/\")[-1]\n",
    "          \n",
    "    print(f\"loaded {len(documents)} documents from {len(file_paths)} files.\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,  # chunk size (characters)\n",
    "        chunk_overlap=200,  # chunk overlap (characters)\n",
    "        add_start_index=True,  # track index in original document\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"generated {len(chunks)} chunks.\")\n",
    "\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=mask_pii(chunk.page_content),\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"source_name\": chunk.metadata[\"source\"],\n",
    "                \"total_pages\": chunk.metadata[\"total_pages\"],\n",
    "                \"start_index\": chunk.metadata[\"start_index\"],\n",
    "                **metadata.model_dump(),\n",
    "            },\n",
    "        )\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    vectorstore = get_vectorstore(collection_name)\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(docs))]\n",
    "    vectorstore.add_documents(docs, ids=ids)\n",
    "    success_message = f\"Ingested {len(docs)} documents into {collection_name} index.\"\n",
    "    print(success_message)\n",
    "    return success_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a1751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List\n",
    "\n",
    "find_dotenv()\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "\n",
    "\n",
    "def reranker(query: str, docs: List[Document]) -> List[Document]:\n",
    "    print(f\"Retrieved {len(docs)} documents\")\n",
    "    retriever = BM25Retriever.from_documents(docs)\n",
    "    result = retriever.invoke(query)\n",
    "    print(\"RERANKER Result: \", len(result), result[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def retrieval(\n",
    "    query: str, collection_name: str, filter_data: MetaData\n",
    ") -> List[tuple[Document, float]]:\n",
    "    vectorstore = get_vectorstore(collection_name)\n",
    "    print(\n",
    "        f\"RETRIEVAL query: {query[:40]}, for {collection_name} collection, with filters: {filter_data}\"\n",
    "    )\n",
    "\n",
    "    filters = [f'language == \"{filter_data.language}\"']\n",
    "    if filter_data.doc_type:\n",
    "        filters.append(f'doc_type == \"{filter_data.doc_type}\"')\n",
    "    if filter_data.domain:\n",
    "        filters.append(f'domain == \"{filter_data.domain}\"')\n",
    "    if filter_data.section:\n",
    "        filters.append(f'section == \"{filter_data.section}\"')\n",
    "    if filter_data.topic:\n",
    "        filters.append(f'topic == \"{filter_data.topic}\"')\n",
    "\n",
    "    expr = \" and \".join(filters) if filters else None\n",
    "    try:\n",
    "        results = vectorstore.similarity_search_with_relevance_scores(\n",
    "            query, k=5, expr=expr\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in retrieval: {str(e)}\")\n",
    "        return []\n",
    "    docs = []\n",
    "    for doc, score in results:\n",
    "        doc.metadata[\"similarity_score\"] = score\n",
    "        docs.append(doc)\n",
    "    # docs = reranker(query, docs)\n",
    "    print(\"RETRIEVED DOCS: \", len(docs))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def generate(query: str, ctx_docs: List[Document]) -> str:\n",
    "    context = \"\\n\".join([doc.page_content for doc in ctx_docs])\n",
    "    prompt = f\"\"\"Answer shortly to the user question according to the given context. Only answer if the context is given to you.\n",
    "    question: {query}\n",
    "    context: {context}\n",
    "\"\"\"\n",
    "    output = model.invoke(prompt)\n",
    "    return output.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1e93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hier-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
